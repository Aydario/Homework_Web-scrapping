# Домашнее задание к лекции 6. «Web-scrapping»
С помощью скрипта получаем ссылку, заголовок, автора и время публикации интересующих нас статей на [Хабре](https://habr.com).

## Пояснение
Парсим страницу со свежими статьями ([вот эту](https://habr.com/ru/all/)) и находим ссылки на статьи. Далее переходим на по каждой ссылке поочередно и парсим страницу статьи. Если встречается хотя бы одно из ключевых слов (дизайн, фото, web, python) в тексте статьи, то выводим в консоль данные статьи в формате:
- "№ п/п" article:
- link: "ссылка на статью"
- title: "заголовок статьи"
- author: "ник автора"
- time: "время публикации"

Также создается json-файл ("articles.json") где сохраняется информация по всем подходящим нам статьям.

## Обновления с учетом исправлений и рекомендаций преподавателя:
- Обработка ошибок запросов: В текущей реализации нет обработки ошибок при запросах (например, если response.status_code != 200) `исправлено`
- Чувствительность к регистру: Поиск ключевых слов идет только в тексте статьи в нижнем регистре. Однако ключевые слова из KEYWORDS не приводятся к нижнему регистру. Это может привести к пропуску результатов `исправлено`
- Оптимизация обработки текста: Вместо построчного анализа текста статьи можно использовать регулярные выражения с флагом re.IGNORECASE `исправлено`
- Логирование: Добавление логов поможет отследить, какие статьи обработаны и были ли в них найдены ключевые слова `исправлено`
- Асинхронные запросы: Для ускорения работы при обработке множества статей можно использовать библиотеку aiohttp `исправлено`

### Исправленная версия находится в файле `version_with_aiohttp.py`
Формат вывода:
- "№ п/п" article:
- found keyword "найденное ключевое слово" in article
- link: "ссылка на статью"
- title: "заголовок статьи"
- author: "ник автора"
- time: "время публикации"

Создается json-файл ("articles_with_aiohttp.json") где сохраняется информация по всем подходящим нам статьям.
